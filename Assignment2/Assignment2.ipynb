{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\tCreate Python files:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = codecs.open(\"itwiki-latest-pages-articles.xml/smallCorpus.xml\", encoding=\"UTF-8\") \n",
    "contents = F.read() # contents if of type String\n",
    "# print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## strip away all HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srn1\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\srn1\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# nltk.clean_html() does not work; have to use BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(contents)\n",
    "text = soup.get_text()\n",
    "# print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### translate the most important html entities like this:\n",
    "### •\t&gt  ––>  >\n",
    "### •\t&lt  ––>  <\n",
    "### •\t&quot  ––>  \"\n",
    "### •\t&amp  ––>  &\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = r\">\"\n",
    "text = re.sub(regex, r\"&gt\", text)\n",
    "\n",
    "regex = r\"<\"\n",
    "text = re.sub(regex, r\"&lt\", text)\n",
    "\n",
    "regex = r'\"'\n",
    "text = re.sub(regex, r\"&quot\", text)\n",
    "\n",
    "regex = r\"&\"\n",
    "text = re.sub(regex, r\"&amp\", text)\n",
    "# print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the wiki formatting, which is described at the wikipedia cheatsheet and the wiki markup help page. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = r\"={1,4}\"\n",
    "text = re.sub(regex, r\" \", text)\n",
    "\n",
    "regex = r\"#{1,3}\"\n",
    "text = re.sub(regex, \" \", text)\n",
    "\n",
    "regex = r\":{1,3}\"\n",
    "text = re.sub(regex, \" \", text)\n",
    "\n",
    "regex = r\"'{2,3}\"\n",
    "text = re.sub(regex, \" \", text)\n",
    "\n",
    "regex = r\"(\\{)(\\{)(([\\w\\s](.)+))(\\})(\\})\"\n",
    "text = re.sub(regex, \" \", text)\n",
    "\n",
    "regex = r\"\\[([A-Za-z0-9_|.]+)\\]\"\n",
    "text = re.sub(regex, \" \", text)\n",
    "\n",
    "regex = r\"([[])\"\n",
    "text = re.sub(regex, \"\", text)\n",
    "\n",
    "regex = r\"([]])\"\n",
    "text = re.sub(regex, \"\", text)\n",
    "\n",
    "regex = r\"s\" # s does mean anything in italian\n",
    "text = re.sub(regex, \"\", text)\n",
    "\n",
    "regex = r\"d\" # d does mean anything in italian\n",
    "text = re.sub(regex, \"\", text)\n",
    "\n",
    "regex = r\"en\" # en does mean anything in italian\n",
    "text = re.sub(regex, \"\", text)\n",
    "\n",
    "regex = r\"n\" # n does mean anything in italian\n",
    "text = re.sub(regex, \"\", text)\n",
    "\n",
    "regex = r\"it\" # n does mean anything in italian\n",
    "text = re.sub(regex, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = regexp_tokenize(text, pattern='\\w+')\n",
    "# print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big is the corpus (# of words)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6987\n"
     ]
    }
   ],
   "source": [
    "wordcount = len(re.findall(r'\\w+', text))\n",
    "print (wordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the average word length? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the average word length? :  5.722415472380687\n"
     ]
    }
   ],
   "source": [
    "w = [len(word) for line in tokens for word in line.rstrip().split(\" \")]\n",
    "w_avg = float(sum(w))/float(len(w))\n",
    "print ('What is the average word length? : ', w_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the 10 most frequent words? How many percent of the corpus consists of these words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the 10 most frequent words? :  [['i', 4113], ['a', 3905], ['e', 3666], ['o', 3420], ['l', 2525], ['at', 524], ['al', 452], ['le', 398], ['ti', 369], ['to', 337]]\n",
      "Percent of the corpus:  46.338137446218234\n"
     ]
    }
   ],
   "source": [
    "listOfWordsAndCounts = []\n",
    "toks = set(tokens) # get only the unique tokens. Convert tokens to set and then convert back to list\n",
    "\n",
    "for word in toks:\n",
    "    wordList = re.findall(word, text) # returns a list of all the occurences of the word in text\n",
    "    wordLenght = len(wordList) #find length of wordList\n",
    "    listOfWordsAndCounts.append([word, wordLenght]) #add word and wordLenght \n",
    "\n",
    "listOfWordsAndCounts.sort(key=lambda x: int(x[1]))\n",
    "print ('What are the 10 most frequent words? : ',listOfWordsAndCounts[:-11:-1])\n",
    "\n",
    "topTen = listOfWordsAndCounts[:-11:-1]\n",
    "percent =  sum(x[1] for x in topTen) / len(text) \n",
    "print('Percent of the corpus: ', percent*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many hapax words are there? How many percent of the corpus are they?\n",
    "Note:  hapax is a word that occurs only once within a context, document, language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1839\n"
     ]
    }
   ],
   "source": [
    "tokens_once = set(word for word in set(tokens) if tokens.count(word) == 1)\n",
    "counts = Counter(tokens_once)\n",
    "print (len (counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is the longest word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pjvi0bis0bzt1nv8dmz9deanoznepsj\n"
     ]
    }
   ],
   "source": [
    " print (max(tokens, key=len))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
